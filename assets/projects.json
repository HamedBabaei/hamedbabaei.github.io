[
    {
        "logo": "img/LLMs4OL.jpg",
        "title": "LLMs4OL: Large Language Models for Ontology Learning",
        "description": "We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text? To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS. The obtained empirical results show that foundational LLMs are not sufficiently suitable for ontology construction that entails a high degree of reasoning skills and domain expertise. Nevertheless, when effectively fine-tuned they just might work as suitable assistants, alleviating the knowledge acquisition bottleneck, for ontology construction.",
        "github": "https://github.com/HamedBabaei/LLMs4OL",
        "paper": "https://link.springer.com/chapter/10.1007/978-3-031-47240-4_22",
        "year": 2023,
        "conference": "The 22nd International Conference on Semantic Web - ISWC 2023"
    },
       {
        "logo": "img/LLMs4OM.png",
        "title": "LLMs4OM: Matching Ontologies with Large Language Models",
        "description": "Ontology Matching (OM), is a critical task in knowledge integration, where aligning heterogeneous ontologies facilitates data interoperability and knowledge sharing. Traditional OM systems often rely on expert knowledge or predictive models, with limited exploration of the potential of Large Language Models (LLMs). We present the LLMs4OM framework, a novel approach to evaluate the effectiveness of LLMs in OM tasks. This framework utilizes two modules for retrieval and matching, respectively, enhanced by zero-shot prompting across three ontology representations: concept, concept-parent, and concept-children. Through comprehensive evaluations using 20 OM datasets from various domains, we demonstrate that LLMs, under the LLMs4OM framework, can match and even surpass the performance of traditional OM systems, particularly in complex matching scenarios. Our results highlight the potential of LLMs to significantly contribute to the field of OM.",
        "github": "https://github.com/HamedBabaei/LLMs4OM",
        "paper": "https://arxiv.org/abs/2404.10317",
        "year": 2024,
        "conference": "The 21st Extended Semantic Web Conference - ESWC 2024"
    }
]