[
    {
        "logo": "img/LLMs4OL.jpg",
        "title": "LLMs4OL: Large Language Models for Ontology Learning",
        "description": "We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text? To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS. The obtained empirical results show that foundational LLMs are not sufficiently suitable for ontology construction that entails a high degree of reasoning skills and domain expertise. Nevertheless, when effectively fine-tuned they just might work as suitable assistants, alleviating the knowledge acquisition bottleneck, for ontology construction.",
        "github": "https://github.com/HamedBabaei/LLMs4OL",
        "paper": "https://link.springer.com/chapter/10.1007/978-3-031-47240-4_22",
        "year": 2023,
        "conference": "The 22nd International Conference on Semantic Web - ISWC 2023",
        "ID": "LLMs4OL"
    },
       {
        "logo": "img/LLMs4OM.png",
        "title": "LLMs4OM: Matching Ontologies with Large Language Models",
        "description": "Ontology Matching (OM), is a critical task in knowledge integration, where aligning heterogeneous ontologies facilitates data interoperability and knowledge sharing. Traditional OM systems often rely on expert knowledge or predictive models, with limited exploration of the potential of Large Language Models (LLMs). We present the LLMs4OM framework, a novel approach to evaluate the effectiveness of LLMs in OM tasks. This framework utilizes two modules for retrieval and matching, respectively, enhanced by zero-shot prompting across three ontology representations: concept, concept-parent, and concept-children. Through comprehensive evaluations using 20 OM datasets from various domains, we demonstrate that LLMs, under the LLMs4OM framework, can match and even surpass the performance of traditional OM systems, particularly in complex matching scenarios. Our results highlight the potential of LLMs to significantly contribute to the field of OM.",
        "github": "https://github.com/HamedBabaei/LLMs4OM",
        "paper": "https://arxiv.org/abs/2404.10317",
        "year": 2024,
        "conference": "The 21st Extended Semantic Web Conference - ESWC 2024",
        "ID": "LLMs4OM"
    },
    {
        "logo": "img/llms4synthesis-logo.png",
        "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
        "description": "In response to the growing complexity and volume of scientific literature, this paper introduces the LLMs4Synthesis framework, designed to enhance the capabilities of Large Language Models (LLMs) in generating high-quality scientific syntheses. This framework addresses the need for rapid, coherent, and contextually rich integration of scientific insights, leveraging both open-source and proprietary LLMs. It also examines the effectiveness of LLMs in evaluating the integrity and reliability of these syntheses, alleviating inadequacies in current quantitative metrics. Our study contributes to this field by developing a novel methodology for processing scientific papers, defining new synthesis types, and establishing nine detailed quality criteria for evaluating syntheses. The integration of LLMs with reinforcement learning and AI feedback is proposed to optimize synthesis quality, ensuring alignment with established criteria. The LLMs4Synthesis framework and its components are made available, promising to enhance both the generation and evaluation processes in scientific research synthesis.",
        "github": "https://github.com/HamedBabaei/LLMs4Synthesis",
        "paper": "https://arxiv.org/abs/2409.18812",
        "year": 2024,
        "conference": "JCDL 2024 - Research Track",
        "ID": "LLMs4Synthesis"
    }
]